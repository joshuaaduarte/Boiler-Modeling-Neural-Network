{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4313cf99-8ea7-45c4-ac26-4ae8e269b478",
   "metadata": {},
   "source": [
    "### Task 1.1a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "f09d8b24-9e5b-4b71-aeab-7a2e07f10023",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean: [1.100e-02 8.500e+02 9.265e-02]\n",
      "standard deviation: [2.07077080e-03 1.97026246e+02 3.63777177e-02]\n",
      "Normalized input data:\n",
      " [[0.72727273 0.64705882 0.66454398]\n",
      " [0.72727273 0.76470588 0.78456557]\n",
      " [0.72727273 0.88235294 0.90620615]\n",
      " [0.72727273 1.         1.00885051]\n",
      " [0.72727273 1.11764706 1.14786832]\n",
      " [0.72727273 1.23529412 1.24349703]\n",
      " [0.72727273 1.35294118 1.38909876]\n",
      " [0.72727273 1.         1.0270912 ]\n",
      " [0.72727273 0.64705882 0.47468969]\n",
      " [0.72727273 0.88235294 0.64727469]\n",
      " [0.72727273 1.11764706 0.81985969]\n",
      " [0.72727273 1.23529412 0.9004857 ]\n",
      " [0.72727273 1.35294118 0.99190502]\n",
      " [0.72727273 1.         0.73362115]\n",
      " [0.72727273 0.64705882 0.36913114]\n",
      " [0.72727273 0.88235294 0.50339989]\n",
      " [0.72727273 1.11764706 0.63766865]\n",
      " [0.72727273 1.35294118 0.77172153]\n",
      " [0.72727273 1.         0.57053427]\n",
      " [1.         0.64705882 0.91311387]\n",
      " [1.         0.88235294 1.24554776]\n",
      " [1.         1.11764706 1.57798165]\n",
      " [1.         1.35294118 1.91041554]\n",
      " [1.         1.         1.41176471]\n",
      " [1.         0.64705882 0.65267134]\n",
      " [1.         0.88235294 0.89001619]\n",
      " [1.         1.11764706 1.12682137]\n",
      " [1.         1.23529412 1.22396114]\n",
      " [1.         1.35294118 1.36427415]\n",
      " [1.         1.         1.00809498]\n",
      " [1.         0.64705882 0.50728548]\n",
      " [1.         0.88235294 0.69217485]\n",
      " [1.         1.11764706 0.87684835]\n",
      " [1.         1.35294118 1.06141392]\n",
      " [1.         1.         0.78457636]\n",
      " [1.         0.82352941 0.9411333 ]\n",
      " [1.18181818 0.64705882 1.07987048]\n",
      " [1.18181818 0.88235294 1.47263896]\n",
      " [1.18181818 1.11764706 1.86529951]\n",
      " [1.18181818 1.35294118 2.25796006]\n",
      " [1.18181818 1.         1.66896924]\n",
      " [1.18181818 0.64705882 0.77139773]\n",
      " [1.18181818 0.88235294 1.05180788]\n",
      " [1.18181818 1.11764706 1.33232596]\n",
      " [1.18181818 1.23529412 1.43572585]\n",
      " [1.18181818 1.35294118 1.61252024]\n",
      " [1.18181818 1.         1.19212089]\n",
      " [1.18181818 0.64705882 0.59989207]\n",
      " [1.18181818 0.88235294 0.81813276]\n",
      " [1.18181818 1.11764706 1.03626552]\n",
      " [1.18181818 1.35294118 1.25418241]\n",
      " [1.18181818 1.         0.92714517]]\n",
      "output data:\n",
      " [[0.71428571 1.00656383]\n",
      " [0.71428571 0.97965212]\n",
      " [0.71428571 0.96652445]\n",
      " [0.71428571 0.95241221]\n",
      " [0.71292517 0.94158188]\n",
      " [0.71292517 0.93239252]\n",
      " [0.71428571 0.92451592]\n",
      " [0.71292517 0.9527404 ]\n",
      " [0.99863946 1.01050213]\n",
      " [1.         0.96980637]\n",
      " [1.         0.94453561]\n",
      " [1.         0.93534624]\n",
      " [1.         0.92714145]\n",
      " [0.99863946 0.95602232]\n",
      " [1.28571429 1.01279947]\n",
      " [1.28571429 0.97210371]\n",
      " [1.28571429 0.94683295]\n",
      " [1.28571429 0.9291106 ]\n",
      " [1.28571429 0.95799147]\n",
      " [0.71428571 1.07646866]\n",
      " [0.71428571 1.02133246]\n",
      " [0.71428571 0.98720053]\n",
      " [0.71428571 0.96357073]\n",
      " [0.71428571 1.00262553]\n",
      " [1.         1.08171972]\n",
      " [1.         1.02592714]\n",
      " [1.         0.99113883]\n",
      " [1.         0.98260584]\n",
      " [1.         0.96750903]\n",
      " [1.         1.00689202]\n",
      " [1.28571429 1.08532983]\n",
      " [1.28571429 1.02920906]\n",
      " [1.28435374 0.99409255]\n",
      " [1.28571429 0.97013456]\n",
      " [1.28435374 1.00984575]\n",
      " [0.99863946 1.06563833]\n",
      " [0.71428571 1.12307187]\n",
      " [0.71292517 1.05776173]\n",
      " [0.71292517 1.01739416]\n",
      " [0.71428571 0.98982606]\n",
      " [0.71292517 1.0354447 ]\n",
      " [0.99863946 1.12930752]\n",
      " [1.         1.06334099]\n",
      " [1.         1.02231703]\n",
      " [1.         1.00525107]\n",
      " [1.         0.99409255]\n",
      " [0.99863946 1.04069577]\n",
      " [1.28571429 1.13324582]\n",
      " [1.28435374 1.0669511 ]\n",
      " [1.28435374 1.02559895]\n",
      " [1.28571429 0.99737447]\n",
      " [1.28571429 1.04430587]]\n",
      "Stored 'median' (ndarray)\n",
      "Stored 'std' (ndarray)\n",
      "[[0.7272727272727273, 0.6470588235294118, 0.664543982730707], [0.7272727272727273, 0.7647058823529411, 0.7845655693470049], [0.7272727272727273, 0.8823529411764706, 0.906206152185645], [0.7272727272727273, 1.0, 1.008850512682137], [0.7272727272727273, 1.1176470588235294, 1.147868321640583], [0.7272727272727273, 1.2352941176470589, 1.2434970318402592], [0.7272727272727273, 1.3529411764705883, 1.3890987587695631], [0.7272727272727273, 1.0, 1.0270912034538586], [0.7272727272727273, 0.6470588235294118, 0.47468969239071773], [0.7272727272727273, 0.8823529411764706, 0.6472746896923908], [0.7272727272727273, 1.1176470588235294, 0.8198596869940638], [0.7272727272727273, 1.2352941176470589, 0.9004856988667027], [0.7272727272727273, 1.3529411764705883, 0.9919050188882893], [0.7272727272727273, 1.0, 0.733621154883972], [0.7272727272727273, 0.6470588235294118, 0.36913113869400976], [0.7272727272727273, 0.8823529411764706, 0.5033998920669185], [0.7272727272727273, 1.1176470588235294, 0.6376686454398274], [0.7272727272727273, 1.3529411764705883, 0.7717215326497571], [0.7272727272727273, 1.0, 0.5705342687533729], [1.0, 0.6470588235294118, 0.9131138694009714], [1.0, 0.8823529411764706, 1.2455477603885592], [1.0, 1.1176470588235294, 1.5779816513761469], [1.0, 1.3529411764705883, 1.9104155423637346], [1.0, 1.0, 1.411764705882353], [1.0, 0.6470588235294118, 0.6526713437668646], [1.0, 0.8823529411764706, 0.8900161899622235], [1.0, 1.1176470588235294, 1.126821370750135], [1.0, 1.2352941176470589, 1.223961144090664], [1.0, 1.3529411764705883, 1.3642741500269835], [1.0, 1.0, 1.0080949811117108], [1.0, 0.6470588235294118, 0.5072854830005397], [1.0, 0.8823529411764706, 0.6921748515920131], [1.0, 1.1176470588235294, 0.8768483540205074], [1.0, 1.3529411764705883, 1.0614139233675122], [1.0, 1.0, 0.7845763626551538], [1.0, 0.8235294117647058, 0.9411332973556394], [1.1818181818181819, 0.6470588235294118, 1.0798704803022128], [1.1818181818181819, 0.8823529411764706, 1.4726389638424178], [1.1818181818181819, 1.1176470588235294, 1.8652995143011335], [1.1818181818181819, 1.3529411764705883, 2.257960064759849], [1.1818181818181819, 1.0, 1.6689692390717754], [1.1818181818181819, 0.6470588235294118, 0.7713977334052888], [1.1818181818181819, 0.8823529411764706, 1.0518078791149488], [1.1818181818181819, 1.1176470588235294, 1.3323259579060982], [1.1818181818181819, 1.2352941176470589, 1.4357258499730168], [1.1818181818181819, 1.3529411764705883, 1.6125202374527794], [1.1818181818181819, 1.0, 1.1921208850512683], [1.1818181818181819, 0.6470588235294118, 0.5998920669185105], [1.1818181818181819, 0.8823529411764706, 0.8181327576902322], [1.1818181818181819, 1.1176470588235294, 1.036265515380464], [1.1818181818181819, 1.3529411764705883, 1.2541824069077172], [1.1818181818181819, 1.0, 0.9271451699946034]]\n",
      "[[0.72727273 0.64705882 0.66454398]\n",
      " [0.72727273 0.76470588 0.78456557]\n",
      " [0.72727273 0.88235294 0.90620615]\n",
      " [0.72727273 1.         1.00885051]\n",
      " [0.72727273 1.11764706 1.14786832]\n",
      " [0.72727273 1.23529412 1.24349703]\n",
      " [0.72727273 1.35294118 1.38909876]\n",
      " [0.72727273 1.         1.0270912 ]\n",
      " [0.72727273 0.64705882 0.47468969]\n",
      " [0.72727273 0.88235294 0.64727469]\n",
      " [0.72727273 1.11764706 0.81985969]\n",
      " [0.72727273 1.23529412 0.9004857 ]\n",
      " [0.72727273 1.35294118 0.99190502]\n",
      " [0.72727273 1.         0.73362115]\n",
      " [0.72727273 0.64705882 0.36913114]\n",
      " [0.72727273 0.88235294 0.50339989]\n",
      " [0.72727273 1.11764706 0.63766865]\n",
      " [0.72727273 1.35294118 0.77172153]\n",
      " [0.72727273 1.         0.57053427]\n",
      " [1.         0.64705882 0.91311387]\n",
      " [1.         0.88235294 1.24554776]\n",
      " [1.         1.11764706 1.57798165]\n",
      " [1.         1.35294118 1.91041554]\n",
      " [1.         1.         1.41176471]\n",
      " [1.         0.64705882 0.65267134]\n",
      " [1.         0.88235294 0.89001619]\n",
      " [1.         1.11764706 1.12682137]\n",
      " [1.         1.23529412 1.22396114]\n",
      " [1.         1.35294118 1.36427415]\n",
      " [1.         1.         1.00809498]\n",
      " [1.         0.64705882 0.50728548]\n",
      " [1.         0.88235294 0.69217485]\n",
      " [1.         1.11764706 0.87684835]\n",
      " [1.         1.35294118 1.06141392]\n",
      " [1.         1.         0.78457636]\n",
      " [1.         0.82352941 0.9411333 ]\n",
      " [1.18181818 0.64705882 1.07987048]\n",
      " [1.18181818 0.88235294 1.47263896]\n",
      " [1.18181818 1.11764706 1.86529951]\n",
      " [1.18181818 1.35294118 2.25796006]\n",
      " [1.18181818 1.         1.66896924]\n",
      " [1.18181818 0.64705882 0.77139773]\n",
      " [1.18181818 0.88235294 1.05180788]\n",
      " [1.18181818 1.11764706 1.33232596]\n",
      " [1.18181818 1.23529412 1.43572585]\n",
      " [1.18181818 1.35294118 1.61252024]\n",
      " [1.18181818 1.         1.19212089]\n",
      " [1.18181818 0.64705882 0.59989207]\n",
      " [1.18181818 0.88235294 0.81813276]\n",
      " [1.18181818 1.11764706 1.03626552]\n",
      " [1.18181818 1.35294118 1.25418241]\n",
      " [1.18181818 1.         0.92714517]]\n",
      "[[0.7142857142857143, 1.0065638332786346], [0.7142857142857143, 0.9796521168362324], [0.7142857142857143, 0.9665244502789629], [0.7142857142857143, 0.9524122087298983], [0.7129251700680272, 0.941581883820151], [0.7129251700680272, 0.9323925172300624], [0.7142857142857143, 0.9245159172957007], [0.7129251700680272, 0.9527404003938301], [0.998639455782313, 1.0105021332458155], [1.0, 0.9698063669182803], [1.0, 0.9445356087955367], [1.0, 0.935346242205448], [1.0, 0.9271414506071546], [0.998639455782313, 0.9560223170331474], [1.2857142857142856, 1.0127994748933378], [1.2857142857142856, 0.9721037085658024], [1.2857142857142856, 0.9468329504430588], [1.2857142857142856, 0.9291106005907451], [1.2857142857142856, 0.9579914670167378], [0.7142857142857143, 1.0764686576960945], [0.7142857142857143, 1.021332458155563], [0.7142857142857143, 0.9872005251066623], [0.7142857142857143, 0.9635707253035773], [0.7142857142857143, 1.002625533311454], [1.0, 1.0817197243190024], [1.0, 1.0259271414506073], [1.0, 0.9911388250738432], [1.0, 0.9826058418116179], [1.0, 0.9675090252707582], [1.0, 1.0068920249425666], [1.2857142857142856, 1.0853298326222514], [1.2857142857142856, 1.0292090580899247], [1.2843537414965986, 0.9940925500492287], [1.2857142857142856, 0.9701345585822121], [1.2843537414965986, 1.009845749917952], [0.998639455782313, 1.0656383327863472], [0.7142857142857143, 1.123071873974401], [0.7129251700680272, 1.0577617328519857], [0.7129251700680272, 1.017394158188382], [0.7142857142857143, 0.9898260584181163], [0.7129251700680272, 1.0354446997046276], [0.998639455782313, 1.1293075155891041], [1.0, 1.0633409911388252], [1.0, 1.0223170331473581], [1.0, 1.0052510666229078], [1.0, 0.9940925500492287], [0.998639455782313, 1.0406957663275354], [1.2857142857142856, 1.133245815556285], [1.2843537414965986, 1.0669510994420743], [1.2843537414965986, 1.0255989497866755], [1.2857142857142856, 0.9973744666885461], [1.2857142857142856, 1.0443058746307843]]\n",
      "[[0.71428571 1.00656383]\n",
      " [0.71428571 0.97965212]\n",
      " [0.71428571 0.96652445]\n",
      " [0.71428571 0.95241221]\n",
      " [0.71292517 0.94158188]\n",
      " [0.71292517 0.93239252]\n",
      " [0.71428571 0.92451592]\n",
      " [0.71292517 0.9527404 ]\n",
      " [0.99863946 1.01050213]\n",
      " [1.         0.96980637]\n",
      " [1.         0.94453561]\n",
      " [1.         0.93534624]\n",
      " [1.         0.92714145]\n",
      " [0.99863946 0.95602232]\n",
      " [1.28571429 1.01279947]\n",
      " [1.28571429 0.97210371]\n",
      " [1.28571429 0.94683295]\n",
      " [1.28571429 0.9291106 ]\n",
      " [1.28571429 0.95799147]\n",
      " [0.71428571 1.07646866]\n",
      " [0.71428571 1.02133246]\n",
      " [0.71428571 0.98720053]\n",
      " [0.71428571 0.96357073]\n",
      " [0.71428571 1.00262553]\n",
      " [1.         1.08171972]\n",
      " [1.         1.02592714]\n",
      " [1.         0.99113883]\n",
      " [1.         0.98260584]\n",
      " [1.         0.96750903]\n",
      " [1.         1.00689202]\n",
      " [1.28571429 1.08532983]\n",
      " [1.28571429 1.02920906]\n",
      " [1.28435374 0.99409255]\n",
      " [1.28571429 0.97013456]\n",
      " [1.28435374 1.00984575]\n",
      " [0.99863946 1.06563833]\n",
      " [0.71428571 1.12307187]\n",
      " [0.71292517 1.05776173]\n",
      " [0.71292517 1.01739416]\n",
      " [0.71428571 0.98982606]\n",
      " [0.71292517 1.0354447 ]\n",
      " [0.99863946 1.12930752]\n",
      " [1.         1.06334099]\n",
      " [1.         1.02231703]\n",
      " [1.         1.00525107]\n",
      " [1.         0.99409255]\n",
      " [0.99863946 1.04069577]\n",
      " [1.28571429 1.13324582]\n",
      " [1.28435374 1.0669511 ]\n",
      " [1.28435374 1.02559895]\n",
      " [1.28571429 0.99737447]\n",
      " [1.28571429 1.04430587]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "xdata = []\n",
    "ydata = []\n",
    "#xdata.append([ Di(m), qoflux (kW/m^2), mdot (kg/s)])\n",
    "\n",
    "\n",
    "xdata.append([0.008, 550, 0.06157])\n",
    "xdata.append([0.008, 650, 0.07269])\n",
    "xdata.append([0.008, 750, 0.08396])\n",
    "xdata.append([0.008, 850, 0.09347])\n",
    "xdata.append([0.008, 950, 0.10635])\n",
    "xdata.append([0.008, 1050, 0.11521])\n",
    "xdata.append([0.008, 1150, 0.1287])\n",
    "xdata.append([0.008, 850, 0.09516])\n",
    "xdata.append([0.008, 550, 0.04398])\n",
    "xdata.append([0.008, 750, 0.05997])\n",
    "xdata.append([0.008, 950, 0.07596])\n",
    "xdata.append([0.008, 1050, 0.08343])\n",
    "xdata.append([0.008, 1150, 0.0919])\n",
    "xdata.append([0.008, 850, 0.06797])\n",
    "xdata.append([0.008, 550, 0.0342])\n",
    "xdata.append([0.008, 750, 0.04664])\n",
    "xdata.append([0.008, 950, 0.05908])\n",
    "xdata.append([0.008, 1150, 0.0715])\n",
    "xdata.append([0.008, 850, 0.05286])\n",
    "xdata.append([0.011, 550, 0.0846])\n",
    "xdata.append([0.011, 750, 0.1154])\n",
    "xdata.append([0.011, 950, 0.1462])\n",
    "xdata.append([0.011, 1150, 0.177])\n",
    "xdata.append([0.011, 850, 0.1308])\n",
    "xdata.append([0.011, 550, 0.06047])\n",
    "xdata.append([0.011, 750, 0.08246])\n",
    "xdata.append([0.011, 950, 0.1044])\n",
    "xdata.append([0.011, 1050, 0.1134])\n",
    "xdata.append([0.011, 1150, 0.1264])\n",
    "xdata.append([0.011, 850, 0.0934])\n",
    "xdata.append([0.011, 550, 0.047])\n",
    "xdata.append([0.011, 750, 0.06413])\n",
    "xdata.append([0.011, 950, 0.08124])\n",
    "xdata.append([0.011, 1150, 0.09834])\n",
    "xdata.append([0.011, 850, 0.072691])\n",
    "xdata.append([0.011, 700, 0.087196])\n",
    "xdata.append([0.013, 550, 0.10005])\n",
    "xdata.append([0.013, 750, 0.13644])\n",
    "xdata.append([0.013, 950, 0.17282])\n",
    "xdata.append([0.013, 1150, 0.2092])\n",
    "xdata.append([0.013, 850, 0.15463])\n",
    "xdata.append([0.013, 550, 0.07147])\n",
    "xdata.append([0.013, 750, 0.09745])\n",
    "xdata.append([0.013, 950, 0.12344])\n",
    "xdata.append([0.013, 1050, 0.13302])\n",
    "xdata.append([0.013, 1150, 0.1494])\n",
    "xdata.append([0.013, 850, 0.11045])\n",
    "xdata.append([0.013, 550, 0.05558])\n",
    "xdata.append([0.013, 750, 0.0758])\n",
    "xdata.append([0.013, 950, 0.09601])\n",
    "xdata.append([0.013, 1150, 0.1162])\n",
    "xdata.append([0.013, 850, 0.0859])\n",
    "\n",
    "#ydata.append([ exit quality, max wall temperature (deg C)])\n",
    "\n",
    "ydata.append([0.525, 306.7])\n",
    "ydata.append([0.525, 298.5])\n",
    "ydata.append([0.525, 294.5])\n",
    "ydata.append([0.525, 290.2])\n",
    "ydata.append([0.524, 286.9])\n",
    "ydata.append([0.524, 284.1])\n",
    "ydata.append([0.525, 281.7])\n",
    "ydata.append([0.524, 290.3])\n",
    "ydata.append([0.734, 307.9])\n",
    "ydata.append([0.735, 295.5])\n",
    "ydata.append([0.735, 287.8])\n",
    "ydata.append([0.735, 285.0])\n",
    "ydata.append([0.735, 282.5])\n",
    "ydata.append([0.734, 291.3])\n",
    "ydata.append([ 0.945, 308.6])\n",
    "ydata.append([0.945, 296.2])\n",
    "ydata.append([0.945, 288.5])\n",
    "ydata.append([0.945, 283.1])\n",
    "ydata.append([0.945, 291.9])\n",
    "ydata.append([ 0.525, 328.0])\n",
    "ydata.append([0.525, 311.2])\n",
    "ydata.append([0.525, 300.8])\n",
    "ydata.append([0.525, 293.6])\n",
    "ydata.append([0.525, 305.5])\n",
    "ydata.append([0.735, 329.6])\n",
    "ydata.append([0.735, 312.6])\n",
    "ydata.append([0.735, 302.0])\n",
    "ydata.append([0.735, 299.4])\n",
    "ydata.append([0.735, 294.8])\n",
    "ydata.append([0.735, 306.8])\n",
    "ydata.append([ 0.945, 330.7])\n",
    "ydata.append([0.945, 313.6])\n",
    "ydata.append([0.944, 302.9])\n",
    "ydata.append([0.945, 295.6])\n",
    "ydata.append([0.944, 307.7])\n",
    "ydata.append([0.734, 324.7])\n",
    "ydata.append([0.525, 342.2])\n",
    "ydata.append([0.524,  322.3])\n",
    "ydata.append([0.524, 310.0])\n",
    "ydata.append([0.525, 301.6])\n",
    "ydata.append([0.524, 315.5])\n",
    "ydata.append([0.734, 344.1])\n",
    "ydata.append([0.735, 324.0])\n",
    "ydata.append([0.735, 311.5])\n",
    "ydata.append([0.735, 306.3])\n",
    "ydata.append([0.735, 302.9])\n",
    "ydata.append([0.734, 317.1])\n",
    "ydata.append([0.945, 345.3])\n",
    "ydata.append([0.944, 325.1])\n",
    "ydata.append([0.944, 312.5])\n",
    "ydata.append([0.945, 303.9])\n",
    "ydata.append([0.945, 318.2])\n",
    "\n",
    "# calculating the mean and standard deviation using numpy\n",
    "median=np.median(xdata, axis=0)\n",
    "mediany = np.median(ydata, axis=0)\n",
    "\n",
    "std=np.std(xdata, axis=0)\n",
    "print('mean:', median)\n",
    "print('standard deviation:', std)\n",
    "#standardize the data\n",
    "Nx = []\n",
    "Ny = []\n",
    "for i in range(len(xdata)):\n",
    "    Nx.append([ (xdata[i][0]/median[0]) , (xdata[i][1]/median[1]) , (xdata[i][2]/median[2])])\n",
    "xdata = Nx\n",
    "for i in range(len(ydata)):\n",
    "    Ny.append([ (ydata[i][0]/mediany[0]) , (ydata[i][1]/mediany[1])])\n",
    "ydata = Ny\n",
    "\n",
    "\n",
    "xarray= np.array(xdata)\n",
    "yarray= np.array(ydata)\n",
    "print('Normalized input data:\\n', xarray)\n",
    "print('output data:\\n', yarray)\n",
    "%store median\n",
    "%store std\n",
    "\n",
    "print (xdata)\n",
    "print (xarray)\n",
    "print (ydata)\n",
    "print (yarray)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7f97b41-6432-49db-b3f8-8ff8857bcb1a",
   "metadata": {},
   "source": [
    "### Task 1.1b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "e9fd6503-33cb-4295-a925-4d05f034467a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training input array:\n",
      "[[0.72727273 1.35294118 0.77172153]\n",
      " [0.72727273 0.88235294 0.50339989]\n",
      " [1.         0.88235294 0.69217485]\n",
      " [1.18181818 1.35294118 2.25796006]\n",
      " [0.72727273 1.23529412 1.24349703]\n",
      " [0.72727273 1.         1.00885051]\n",
      " [0.72727273 1.         1.0270912 ]\n",
      " [1.18181818 1.11764706 1.33232596]\n",
      " [0.72727273 1.35294118 0.99190502]\n",
      " [0.72727273 0.64705882 0.47468969]\n",
      " [1.         1.23529412 1.22396114]\n",
      " [1.         0.64705882 0.65267134]\n",
      " [1.18181818 1.         0.92714517]\n",
      " [0.72727273 0.76470588 0.78456557]\n",
      " [1.         0.64705882 0.50728548]\n",
      " [1.18181818 1.         1.19212089]\n",
      " [1.         1.35294118 1.91041554]\n",
      " [1.18181818 0.64705882 1.07987048]\n",
      " [1.         1.35294118 1.06141392]\n",
      " [1.         1.11764706 1.57798165]\n",
      " [1.         0.64705882 0.91311387]\n",
      " [1.         1.11764706 0.87684835]\n",
      " [1.18181818 0.64705882 0.77139773]\n",
      " [0.72727273 1.23529412 0.9004857 ]\n",
      " [0.72727273 0.88235294 0.64727469]\n",
      " [1.18181818 1.11764706 1.03626552]\n",
      " [0.72727273 1.         0.73362115]\n",
      " [1.18181818 0.88235294 1.47263896]\n",
      " [0.72727273 0.88235294 0.90620615]\n",
      " [1.         1.11764706 1.12682137]\n",
      " [1.         0.82352941 0.9411333 ]\n",
      " [1.18181818 1.23529412 1.43572585]\n",
      " [1.         0.88235294 0.89001619]\n",
      " [1.         1.         0.78457636]\n",
      " [1.18181818 1.11764706 1.86529951]\n",
      " [0.72727273 1.11764706 0.63766865]\n",
      " [0.72727273 1.11764706 0.81985969]\n",
      " [1.18181818 0.88235294 0.81813276]\n",
      " [0.72727273 1.         0.57053427]]\n",
      "validation input array:\n",
      "[[1.18181818 0.88235294 1.05180788]\n",
      " [0.72727273 1.11764706 1.14786832]\n",
      " [1.         0.88235294 1.24554776]\n",
      " [1.18181818 1.35294118 1.25418241]\n",
      " [0.72727273 0.64705882 0.66454398]\n",
      " [0.72727273 0.64705882 0.36913114]\n",
      " [1.18181818 1.         1.66896924]\n",
      " [1.         1.         1.00809498]\n",
      " [1.18181818 1.35294118 1.61252024]\n",
      " [0.72727273 1.35294118 1.38909876]\n",
      " [1.         1.35294118 1.36427415]\n",
      " [1.         1.         1.41176471]\n",
      " [1.18181818 0.64705882 0.59989207]]\n",
      "training output array:\n",
      "[[1.28571429 0.9291106 ]\n",
      " [1.28571429 0.97210371]\n",
      " [1.28571429 1.02920906]\n",
      " [0.71428571 0.98982606]\n",
      " [0.71292517 0.93239252]\n",
      " [0.71428571 0.95241221]\n",
      " [0.71292517 0.9527404 ]\n",
      " [1.         1.02231703]\n",
      " [1.         0.92714145]\n",
      " [0.99863946 1.01050213]\n",
      " [1.         0.98260584]\n",
      " [1.         1.08171972]\n",
      " [1.28571429 1.04430587]\n",
      " [0.71428571 0.97965212]\n",
      " [1.28571429 1.08532983]\n",
      " [0.99863946 1.04069577]\n",
      " [0.71428571 0.96357073]\n",
      " [0.71428571 1.12307187]\n",
      " [1.28571429 0.97013456]\n",
      " [0.71428571 0.98720053]\n",
      " [0.71428571 1.07646866]\n",
      " [1.28435374 0.99409255]\n",
      " [0.99863946 1.12930752]\n",
      " [1.         0.93534624]\n",
      " [1.         0.96980637]\n",
      " [1.28435374 1.02559895]\n",
      " [0.99863946 0.95602232]\n",
      " [0.71292517 1.05776173]\n",
      " [0.71428571 0.96652445]\n",
      " [1.         0.99113883]\n",
      " [0.99863946 1.06563833]\n",
      " [1.         1.00525107]\n",
      " [1.         1.02592714]\n",
      " [1.28435374 1.00984575]\n",
      " [0.71292517 1.01739416]\n",
      " [1.28571429 0.94683295]\n",
      " [1.         0.94453561]\n",
      " [1.28435374 1.0669511 ]\n",
      " [1.28571429 0.95799147]]\n",
      "validation output array:\n",
      "[[1.         1.06334099]\n",
      " [0.71292517 0.94158188]\n",
      " [0.71428571 1.02133246]\n",
      " [1.28571429 0.99737447]\n",
      " [0.71428571 1.00656383]\n",
      " [1.28571429 1.01279947]\n",
      " [0.71292517 1.0354447 ]\n",
      " [1.         1.00689202]\n",
      " [1.         0.99409255]\n",
      " [0.71428571 0.92451592]\n",
      " [1.         0.96750903]\n",
      " [0.71428571 1.00262553]\n",
      " [1.28571429 1.13324582]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train_xarray,valid_xarray,train_yarray,valid_yarray = train_test_split(xarray, yarray, test_size=0.25, random_state=13)\n",
    "\n",
    "print('training input array:')\n",
    "print(train_xarray)\n",
    "print('validation input array:') \n",
    "print(valid_xarray)\n",
    "print('training output array:')\n",
    "print(train_yarray)\n",
    "print('validation output array:') \n",
    "print(valid_yarray)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "8de071b3-b409-4c23-8bed-13f087dcf8c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.72727273 1.35294118 0.77172153]\n",
      " [0.72727273 0.88235294 0.50339989]\n",
      " [1.         0.88235294 0.69217485]\n",
      " [1.18181818 1.35294118 2.25796006]\n",
      " [0.72727273 1.23529412 1.24349703]\n",
      " [0.72727273 1.         1.00885051]\n",
      " [0.72727273 1.         1.0270912 ]\n",
      " [1.18181818 1.11764706 1.33232596]\n",
      " [0.72727273 1.35294118 0.99190502]\n",
      " [0.72727273 0.64705882 0.47468969]\n",
      " [1.         1.23529412 1.22396114]\n",
      " [1.         0.64705882 0.65267134]\n",
      " [1.18181818 1.         0.92714517]\n",
      " [0.72727273 0.76470588 0.78456557]\n",
      " [1.         0.64705882 0.50728548]\n",
      " [1.18181818 1.         1.19212089]\n",
      " [1.         1.35294118 1.91041554]\n",
      " [1.18181818 0.64705882 1.07987048]\n",
      " [1.         1.35294118 1.06141392]\n",
      " [1.         1.11764706 1.57798165]\n",
      " [1.         0.64705882 0.91311387]\n",
      " [1.         1.11764706 0.87684835]\n",
      " [1.18181818 0.64705882 0.77139773]\n",
      " [0.72727273 1.23529412 0.9004857 ]\n",
      " [0.72727273 0.88235294 0.64727469]\n",
      " [1.18181818 1.11764706 1.03626552]\n",
      " [0.72727273 1.         0.73362115]\n",
      " [1.18181818 0.88235294 1.47263896]\n",
      " [0.72727273 0.88235294 0.90620615]\n",
      " [1.         1.11764706 1.12682137]\n",
      " [1.         0.82352941 0.9411333 ]\n",
      " [1.18181818 1.23529412 1.43572585]\n",
      " [1.         0.88235294 0.89001619]\n",
      " [1.         1.         0.78457636]\n",
      " [1.18181818 1.11764706 1.86529951]\n",
      " [0.72727273 1.11764706 0.63766865]\n",
      " [0.72727273 1.11764706 0.81985969]\n",
      " [1.18181818 0.88235294 0.81813276]\n",
      " [0.72727273 1.         0.57053427]]\n",
      "[[1.28571429 0.9291106 ]\n",
      " [1.28571429 0.97210371]\n",
      " [1.28571429 1.02920906]\n",
      " [0.71428571 0.98982606]\n",
      " [0.71292517 0.93239252]\n",
      " [0.71428571 0.95241221]\n",
      " [0.71292517 0.9527404 ]\n",
      " [1.         1.02231703]\n",
      " [1.         0.92714145]\n",
      " [0.99863946 1.01050213]\n",
      " [1.         0.98260584]\n",
      " [1.         1.08171972]\n",
      " [1.28571429 1.04430587]\n",
      " [0.71428571 0.97965212]\n",
      " [1.28571429 1.08532983]\n",
      " [0.99863946 1.04069577]\n",
      " [0.71428571 0.96357073]\n",
      " [0.71428571 1.12307187]\n",
      " [1.28571429 0.97013456]\n",
      " [0.71428571 0.98720053]\n",
      " [0.71428571 1.07646866]\n",
      " [1.28435374 0.99409255]\n",
      " [0.99863946 1.12930752]\n",
      " [1.         0.93534624]\n",
      " [1.         0.96980637]\n",
      " [1.28435374 1.02559895]\n",
      " [0.99863946 0.95602232]\n",
      " [0.71292517 1.05776173]\n",
      " [0.71428571 0.96652445]\n",
      " [1.         0.99113883]\n",
      " [0.99863946 1.06563833]\n",
      " [1.         1.00525107]\n",
      " [1.         1.02592714]\n",
      " [1.28435374 1.00984575]\n",
      " [0.71292517 1.01739416]\n",
      " [1.28571429 0.94683295]\n",
      " [1.         0.94453561]\n",
      " [1.28435374 1.0669511 ]\n",
      " [1.28571429 0.95799147]]\n"
     ]
    }
   ],
   "source": [
    "'''>>>>> start CodeP4.1F22\n",
    "    V.P. Carey ME249, Fall 2022\n",
    "\n",
    "Keras Neural Network Modeling '''\n",
    "\n",
    "#import useful packages\n",
    "import keras\n",
    "import pandas as pd\n",
    "from keras.models import Sequential\n",
    "import numpy as np\n",
    "import keras.backend as kb\n",
    "import tensorflow as tf\n",
    "#the following 2 lines are only needed for Mac OS machines\n",
    "import os\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'\n",
    "\n",
    "xarray= train_xarray\n",
    "\n",
    "print (xarray)\n",
    "\n",
    "yarray= train_yarray\n",
    "print (yarray)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "3f38dcc6-4d67-4548-a453-3252e6d7e863",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define neural network model\n",
    "\n",
    "#As seen below, we have created four dense layers. \n",
    "#A dense layer is a layer in neural network that’s fully connected. \n",
    "#In other words, all the neurons in one layer are connected to all other neurons in the next layer.\n",
    "#In the first layer, we need to provide the input shape, which is 1 in our case. \n",
    "#The activation function we have chosen is elu, which stands for exponential linear unit. .\n",
    "\n",
    "from keras import backend as K\n",
    "#initialize weights with values between -0.2 and 0.5\n",
    "initializer = keras.initializers.RandomUniform(minval= -0.2, maxval=0.5)\n",
    "\n",
    "model = keras.Sequential([\n",
    "    keras.layers.Dense(6, activation=K.elu, input_shape=[3],  kernel_initializer=initializer),\n",
    "    keras.layers.Dense(8, activation=K.elu,  kernel_initializer=initializer),\n",
    "    keras.layers.Dense(16, activation=K.elu,  kernel_initializer=initializer),\n",
    "    keras.layers.Dense(8, activation=K.elu, kernel_initializer=initializer),\n",
    "    keras.layers.Dense(2,  kernel_initializer=initializer)\n",
    "  ])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "241f3dda-708a-45c9-a53e-1827e1dbe6c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#We’re using RMSprop as our optimizer here. RMSprop stands for Root Mean Square Propagation. \n",
    "#It’s one of the most popular gradient descent optimization algorithms for deep learning networks. \n",
    "#RMSprop is an optimizer that’s reliable and fast.\n",
    "#We’re compiling the mode using the model.compile function. The loss function used here \n",
    "#is mean squared error. After the compilation of the model, we’ll use the fit method with ~500 epochs.\n",
    "#Number of epochs can be varied.\n",
    "#from tf.keras import optimizers\n",
    "import tensorflow\n",
    "rms = tensorflow.keras.optimizers.RMSprop(0.02)\n",
    "model.compile(loss='mean_absolute_error',optimizer=rms)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "c52e9ae7-9c55-49a0-9319-40da147a2233",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/800\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.0671\n",
      "Epoch 2/800\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 0.0690\n",
      "Epoch 3/800\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.0371\n",
      "Epoch 4/800\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.0369\n",
      "Epoch 5/800\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.0544\n",
      "Epoch 6/800\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.0760\n",
      "Epoch 7/800\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 0.0425\n",
      "Epoch 8/800\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 0.0590\n",
      "Epoch 9/800\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 0.0383\n",
      "Epoch 10/800\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.0869\n",
      "Epoch 11/800\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.0476\n",
      "Epoch 12/800\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.0350\n",
      "Epoch 13/800\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 0.0707\n",
      "Epoch 14/800\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.0481\n",
      "Epoch 15/800\n",
      "2/2 [==============================] - 0s 1000us/step - loss: 0.0470\n",
      "Epoch 16/800\n",
      "2/2 [==============================] - 0s 998us/step - loss: 0.0629\n",
      "Epoch 17/800\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.0770\n",
      "Epoch 18/800\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 0.0478\n",
      "Epoch 19/800\n",
      "2/2 [==============================] - 0s 999us/step - loss: 0.0585\n",
      "Epoch 20/800\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.0488\n",
      "Epoch 21/800\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.0331\n",
      "Epoch 22/800\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 0.0743\n",
      "Epoch 23/800\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.0453\n",
      "Epoch 24/800\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 0.0692\n",
      "Epoch 25/800\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 0.0403\n",
      "Epoch 26/800\n",
      "2/2 [==============================] - 0s 1000us/step - loss: 0.0637\n",
      "Epoch 27/800\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 0.0696\n",
      "Epoch 28/800\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.0588\n",
      "Epoch 29/800\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.0323\n",
      "Epoch 30/800\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 0.0502\n",
      "Epoch 31/800\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.0633\n",
      "Epoch 32/800\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 0.0636\n",
      "Epoch 33/800\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 0.0341\n",
      "Epoch 34/800\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.0425\n",
      "Epoch 35/800\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 0.0620\n",
      "Epoch 36/800\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.0721\n",
      "Epoch 37/800\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.0313\n",
      "Epoch 38/800\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 0.0663\n",
      "Epoch 39/800\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 0.0440\n",
      "Epoch 40/800\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 0.0368\n",
      "Epoch 41/800\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.0562\n",
      "Epoch 42/800\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 0.0597\n",
      "Epoch 43/800\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.0351\n",
      "Epoch 44/800\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 0.0376\n",
      "Epoch 45/800\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.0919\n",
      "Epoch 46/800\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.0280\n",
      "Epoch 47/800\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.0785\n",
      "Epoch 48/800\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 0.0571\n",
      "Epoch 49/800\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.0782\n",
      "Epoch 50/800\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.0478\n",
      "Epoch 51/800\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 0.0478\n",
      "Epoch 52/800\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 0.0419\n",
      "Epoch 53/800\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.0762\n",
      "Epoch 54/800\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.0483\n",
      "Epoch 55/800\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 0.0941\n",
      "Epoch 56/800\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.0427\n",
      "Epoch 57/800\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.0320\n",
      "Epoch 58/800\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 0.0585\n",
      "Epoch 59/800\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.0452\n",
      "Epoch 60/800\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.0440\n",
      "Epoch 61/800\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.0546\n",
      "Epoch 62/800\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 0.0478\n",
      "Epoch 63/800\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 0.0468\n",
      "Epoch 64/800\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 0.0343\n",
      "Epoch 65/800\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 0.0732\n",
      "Epoch 66/800\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.0654\n",
      "Epoch 67/800\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 0.0611\n",
      "Epoch 68/800\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 0.0808\n",
      "Epoch 69/800\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.0414\n",
      "Epoch 70/800\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 0.0333\n",
      "Epoch 71/800\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.0337\n",
      "Epoch 72/800\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 0.0544\n",
      "Epoch 73/800\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 0.0538\n",
      "Epoch 74/800\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 0.0630\n",
      "Epoch 75/800\n",
      "2/2 [==============================] - 0s 1000us/step - loss: 0.0558\n",
      "Epoch 76/800\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 0.0470\n",
      "Epoch 77/800\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.0314\n",
      "Epoch 78/800\n",
      "2/2 [==============================] - 0s 999us/step - loss: 0.0585\n",
      "Epoch 79/800\n",
      "2/2 [==============================] - 0s 1000us/step - loss: 0.0767\n",
      "Epoch 80/800\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.0486\n",
      "Epoch 81/800\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.0398\n",
      "Epoch 82/800\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.0500\n",
      "Epoch 83/800\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.0477\n",
      "Epoch 84/800\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.0266\n",
      "Epoch 85/800\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 0.0368\n",
      "Epoch 86/800\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 0.0531\n",
      "Epoch 87/800\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.0430\n",
      "Epoch 88/800\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 0.0431\n",
      "Epoch 89/800\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 0.0642\n",
      "Epoch 90/800\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.0496\n",
      "Epoch 91/800\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.0529\n",
      "Epoch 92/800\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 0.0430\n",
      "Epoch 93/800\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.0715\n",
      "Epoch 94/800\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.0482\n",
      "Epoch 95/800\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.0470\n",
      "Epoch 96/800\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.0339\n",
      "Epoch 97/800\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.0565\n",
      "Epoch 98/800\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 0.0568\n",
      "Epoch 99/800\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.0556\n",
      "Epoch 100/800\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 0.0459\n",
      "Epoch 101/800\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 0.0493\n",
      "Epoch 102/800\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.0465\n",
      "Epoch 103/800\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 0.0765\n",
      "Epoch 104/800\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.0399\n",
      "Epoch 105/800\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.0666\n",
      "Epoch 106/800\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 0.0534\n",
      "Epoch 107/800\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.0546\n",
      "Epoch 108/800\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.0725\n",
      "Epoch 109/800\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 0.0397\n",
      "Epoch 110/800\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.0538\n",
      "Epoch 111/800\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.0295\n",
      "Epoch 112/800\n",
      "2/2 [==============================] - 0s 999us/step - loss: 0.0479\n",
      "Epoch 113/800\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.0591\n",
      "Epoch 114/800\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.0424\n",
      "Epoch 115/800\n",
      "2/2 [==============================] - 0s 1000us/step - loss: 0.0676\n",
      "Epoch 116/800\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 0.0419\n",
      "Epoch 117/800\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 0.0598\n",
      "Epoch 118/800\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 0.0492\n",
      "Epoch 119/800\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.0667\n",
      "Epoch 120/800\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 0.0759\n",
      "Epoch 121/800\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.0523\n",
      "Epoch 122/800\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 0.0567\n",
      "Epoch 123/800\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.0502\n",
      "Epoch 124/800\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 0.0421\n",
      "Epoch 125/800\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.0524\n",
      "Epoch 126/800\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.0584\n",
      "Epoch 127/800\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.0384\n",
      "Epoch 128/800\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.0502\n",
      "Epoch 129/800\n",
      "2/2 [==============================] - 0s 999us/step - loss: 0.0440\n",
      "Epoch 130/800\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.0246\n",
      "Epoch 131/800\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 0.0800\n",
      "Epoch 132/800\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.0626\n",
      "Epoch 133/800\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.0461\n",
      "Epoch 134/800\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.0508\n",
      "Epoch 135/800\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.0375\n",
      "Epoch 136/800\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.0397\n",
      "Epoch 137/800\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.0487\n",
      "Epoch 138/800\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.0682\n",
      "Epoch 139/800\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.0405\n",
      "Epoch 140/800\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.0249\n",
      "Epoch 141/800\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.0524\n",
      "Epoch 142/800\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 0.0375\n",
      "Epoch 143/800\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 0.0718\n",
      "Epoch 144/800\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.0741\n",
      "Epoch 145/800\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.0394\n",
      "Epoch 146/800\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 0.0666\n",
      "Epoch 147/800\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.0329\n",
      "Epoch 148/800\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 0.0669\n",
      "Epoch 149/800\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.0484\n",
      "Epoch 150/800\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.0363\n",
      "Epoch 151/800\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.0450\n",
      "Epoch 152/800\n",
      "2/2 [==============================] - 0s 999us/step - loss: 0.0420\n",
      "Epoch 153/800\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.0414\n",
      "Epoch 154/800\n",
      "2/2 [==============================] - 0s 1000us/step - loss: 0.0585\n",
      "Epoch 155/800\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 0.0728\n",
      "Epoch 156/800\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.0464\n",
      "Epoch 157/800\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.0563\n",
      "Epoch 158/800\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.0426\n",
      "Epoch 159/800\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.0894\n",
      "Epoch 160/800\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 0.0484\n",
      "Epoch 161/800\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.0646\n",
      "Epoch 162/800\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.0378\n",
      "Epoch 163/800\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 0.0397\n",
      "Epoch 164/800\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.0538\n",
      "Epoch 165/800\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.0512\n",
      "Epoch 166/800\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 0.0324\n",
      "Epoch 167/800\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 0.0336\n",
      "Epoch 168/800\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 0.0406\n",
      "Epoch 169/800\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.0672\n",
      "Epoch 170/800\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.0356\n",
      "Epoch 171/800\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 0.0690\n",
      "Epoch 172/800\n",
      "2/2 [==============================] - 0s 1000us/step - loss: 0.0485\n",
      "Epoch 173/800\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 0.0413\n",
      "Epoch 174/800\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.0613\n",
      "Epoch 175/800\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.0714\n",
      "Epoch 176/800\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 0.0498\n",
      "Epoch 177/800\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.0401\n",
      "Epoch 178/800\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 0.0597\n",
      "Epoch 179/800\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.0452\n",
      "Epoch 180/800\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.0452\n",
      "Epoch 181/800\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.0467\n",
      "Epoch 182/800\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.0395\n",
      "Epoch 183/800\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 0.0463\n",
      "Epoch 184/800\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.0231\n",
      "Epoch 185/800\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 0.0457\n",
      "Epoch 186/800\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.0448\n",
      "Epoch 187/800\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.0640\n",
      "Epoch 188/800\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.0603\n",
      "Epoch 189/800\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 0.0409\n",
      "Epoch 190/800\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 0.0654\n",
      "Epoch 191/800\n",
      "2/2 [==============================] - 0s 1000us/step - loss: 0.0394\n",
      "Epoch 192/800\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.0594\n",
      "Epoch 193/800\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.0440\n",
      "Epoch 194/800\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.0422\n",
      "Epoch 195/800\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 0.0521\n",
      "Epoch 196/800\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.0553\n",
      "Epoch 197/800\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.0603\n",
      "Epoch 198/800\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.0480\n",
      "Epoch 199/800\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.0759\n",
      "Epoch 200/800\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 0.0317\n",
      "Epoch 201/800\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.0625\n",
      "Epoch 202/800\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.0549\n",
      "Epoch 203/800\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 0.0316\n",
      "Epoch 204/800\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.0497\n",
      "Epoch 205/800\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.0508\n",
      "Epoch 206/800\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.0443\n",
      "Epoch 207/800\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.0801\n",
      "Epoch 208/800\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.0392\n",
      "Epoch 209/800\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.0702\n",
      "Epoch 210/800\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.0673\n",
      "Epoch 211/800\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 0.0497\n",
      "Epoch 212/800\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 0.0427\n",
      "Epoch 213/800\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.0593\n",
      "Epoch 214/800\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 0.0366\n",
      "Epoch 215/800\n",
      "2/2 [==============================] - 0s 1000us/step - loss: 0.0492\n",
      "Epoch 216/800\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 0.0406\n",
      "Epoch 217/800\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.0375\n",
      "Epoch 218/800\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 0.0385\n",
      "Epoch 219/800\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 0.0463\n",
      "Epoch 220/800\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.0541\n",
      "Epoch 221/800\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 0.0443\n",
      "Epoch 222/800\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 0.0564\n",
      "Epoch 223/800\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 0.0627\n",
      "Epoch 224/800\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 0.0315\n",
      "Epoch 225/800\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 0.0779\n",
      "Epoch 226/800\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 0.0579\n",
      "Epoch 227/800\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.0442\n",
      "Epoch 228/800\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 0.0343\n",
      "Epoch 229/800\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.0517\n",
      "Epoch 230/800\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.0404\n",
      "Epoch 231/800\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.0674\n",
      "Epoch 232/800\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 0.0764\n",
      "Epoch 233/800\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.0253\n",
      "Epoch 234/800\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.0861\n",
      "Epoch 235/800\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.0558\n",
      "Epoch 236/800\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 0.0426\n",
      "Epoch 237/800\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.0509\n",
      "Epoch 238/800\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.0395\n",
      "Epoch 239/800\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.0669\n",
      "Epoch 240/800\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.0361\n",
      "Epoch 241/800\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.0431\n",
      "Epoch 242/800\n",
      "2/2 [==============================] - 0s 1000us/step - loss: 0.0403\n",
      "Epoch 243/800\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.0445\n",
      "Epoch 244/800\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 0.0301\n",
      "Epoch 245/800\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 0.0732\n",
      "Epoch 246/800\n",
      "2/2 [==============================] - 0s 1000us/step - loss: 0.0313\n",
      "Epoch 247/800\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 0.0707\n",
      "Epoch 248/800\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.0573\n",
      "Epoch 249/800\n",
      "2/2 [==============================] - 0s 999us/step - loss: 0.0624\n",
      "Epoch 250/800\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 0.0434\n",
      "Epoch 251/800\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.0519\n",
      "Epoch 252/800\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 0.0489\n",
      "Epoch 253/800\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.0767\n",
      "Epoch 254/800\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.0397\n",
      "Epoch 255/800\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 0.0593\n",
      "Epoch 256/800\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.0403\n",
      "Epoch 257/800\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.0669\n",
      "Epoch 258/800\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 0.0595\n",
      "Epoch 259/800\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.0435\n",
      "Epoch 260/800\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.0374\n",
      "Epoch 261/800\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 0.0482\n",
      "Epoch 262/800\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.0206\n",
      "Epoch 263/800\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 0.0328\n",
      "Epoch 264/800\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 0.0607\n",
      "Epoch 265/800\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.0508\n",
      "Epoch 266/800\n",
      "2/2 [==============================] - 0s 1000us/step - loss: 0.0743\n",
      "Epoch 267/800\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.0258\n",
      "Epoch 268/800\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.0834\n",
      "Epoch 269/800\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.0827\n",
      "Epoch 270/800\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 0.0553\n",
      "Epoch 271/800\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.0371\n",
      "Epoch 272/800\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.0312\n",
      "Epoch 273/800\n",
      "2/2 [==============================] - 0s 999us/step - loss: 0.0529\n",
      "Epoch 274/800\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 0.0623\n",
      "Epoch 275/800\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.0414\n",
      "Epoch 276/800\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 0.0618\n",
      "Epoch 277/800\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.0644\n",
      "Epoch 278/800\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 0.0500\n",
      "Epoch 279/800\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 0.0573\n",
      "Epoch 280/800\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.0513\n",
      "Epoch 281/800\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 0.0437\n",
      "Epoch 282/800\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 0.0492\n",
      "Epoch 283/800\n",
      "2/2 [==============================] - 0s 1000us/step - loss: 0.0602\n",
      "Epoch 284/800\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 0.0448\n",
      "Epoch 285/800\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 0.0316\n",
      "Epoch 286/800\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 0.0648\n",
      "Epoch 287/800\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.0370\n",
      "Epoch 288/800\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 0.0643\n",
      "Epoch 289/800\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 0.0433\n",
      "Epoch 290/800\n",
      "2/2 [==============================] - 0s 1000us/step - loss: 0.0489\n",
      "Epoch 291/800\n",
      "2/2 [==============================] - 0s 1000us/step - loss: 0.0508\n",
      "Epoch 292/800\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 0.0507\n",
      "Epoch 293/800\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 0.0741\n",
      "Epoch 294/800\n",
      "2/2 [==============================] - 0s 1000us/step - loss: 0.0451\n",
      "Epoch 295/800\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.0417\n",
      "Epoch 296/800\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.0388\n",
      "Epoch 297/800\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.0314\n",
      "Epoch 298/800\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 0.0596\n",
      "Epoch 299/800\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 0.0523\n",
      "Epoch 300/800\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 0.0481\n",
      "Epoch 301/800\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.0418\n",
      "Epoch 302/800\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 0.0375\n",
      "Epoch 303/800\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 0.0678\n",
      "Epoch 304/800\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.0341\n",
      "Epoch 305/800\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.0606\n",
      "Epoch 306/800\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.0645\n",
      "Epoch 307/800\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.0598\n",
      "Epoch 308/800\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 0.0535\n",
      "Epoch 309/800\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.0465\n",
      "Epoch 310/800\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.0323\n",
      "Epoch 311/800\n",
      "2/2 [==============================] - 0s 1000us/step - loss: 0.0337\n",
      "Epoch 312/800\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.0466\n",
      "Epoch 313/800\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 0.0496\n",
      "Epoch 314/800\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.0473\n",
      "Epoch 315/800\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.0618\n",
      "Epoch 316/800\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 0.0404\n",
      "Epoch 317/800\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.0506\n",
      "Epoch 318/800\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.0398\n",
      "Epoch 319/800\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.0628\n",
      "Epoch 320/800\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 0.0561\n",
      "Epoch 321/800\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 0.0419\n",
      "Epoch 322/800\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 0.0392\n",
      "Epoch 323/800\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 0.0592\n",
      "Epoch 324/800\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 0.0673\n",
      "Epoch 325/800\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.0498\n",
      "Epoch 326/800\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.0452\n",
      "Epoch 327/800\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.0478\n",
      "Epoch 328/800\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.0421\n",
      "Epoch 329/800\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.0446\n",
      "Epoch 330/800\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 0.0449\n",
      "Epoch 331/800\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 0.0500\n",
      "Epoch 332/800\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 0.0262\n",
      "Epoch 333/800\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.0655\n",
      "Epoch 334/800\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 0.0575\n",
      "Epoch 335/800\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 0.0535\n",
      "Epoch 336/800\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 0.0288\n",
      "Epoch 337/800\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 0.0390\n",
      "Epoch 338/800\n",
      "2/2 [==============================] - 0s 1000us/step - loss: 0.0508\n",
      "Epoch 339/800\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.0374\n",
      "Epoch 340/800\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 0.0299\n",
      "Epoch 341/800\n",
      "2/2 [==============================] - 0s 999us/step - loss: 0.0453\n",
      "Epoch 342/800\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.0507\n",
      "Epoch 343/800\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 0.0662\n",
      "Epoch 344/800\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 0.0704\n",
      "Epoch 345/800\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.0319\n",
      "Epoch 346/800\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 0.0616\n",
      "Epoch 347/800\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 0.0292\n",
      "Epoch 348/800\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 0.0419\n",
      "Epoch 349/800\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.0580\n",
      "Epoch 350/800\n",
      "2/2 [==============================] - 0s 1000us/step - loss: 0.0504\n",
      "Epoch 351/800\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.0392\n",
      "Epoch 352/800\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 0.0402\n",
      "Epoch 353/800\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.0629\n",
      "Epoch 354/800\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.0314\n",
      "Epoch 355/800\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.0285\n",
      "Epoch 356/800\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 0.0359\n",
      "Epoch 357/800\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 0.0679\n",
      "Epoch 358/800\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 0.0514\n",
      "Epoch 359/800\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.0397\n",
      "Epoch 360/800\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 0.0873\n",
      "Epoch 361/800\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 0.0593\n",
      "Epoch 362/800\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 0.0518\n",
      "Epoch 363/800\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 0.0607\n",
      "Epoch 364/800\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 0.0303\n",
      "Epoch 365/800\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.0408\n",
      "Epoch 366/800\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 0.0652\n",
      "Epoch 367/800\n",
      "2/2 [==============================] - 0s 1000us/step - loss: 0.0409\n",
      "Epoch 368/800\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 0.0361\n",
      "Epoch 369/800\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 0.0610\n",
      "Epoch 370/800\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 0.0235\n",
      "Epoch 371/800\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.0547\n",
      "Epoch 372/800\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.0315\n",
      "Epoch 373/800\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 0.0695\n",
      "Epoch 374/800\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.0649\n",
      "Epoch 375/800\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 0.0468\n",
      "Epoch 376/800\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.0347\n",
      "Epoch 377/800\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 0.0554\n",
      "Epoch 378/800\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.0527\n",
      "Epoch 379/800\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 0.0442\n",
      "Epoch 380/800\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.0524\n",
      "Epoch 381/800\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 0.0501\n",
      "Epoch 382/800\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0393Restoring model weights from the end of the best epoch: 262.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.0373\n",
      "Epoch 382: early stopping\n",
      "best epoch =  262\n",
      "smallest loss = 0.02058834210038185\n",
      "INFO:tensorflow:Assets written to: ./best_model\\assets\n"
     ]
    }
   ],
   "source": [
    "#After the compilation of the model, we’ll use the fit method with 500 epochs.\n",
    "#I started with epochs value of 100 and then tested the model after training. \n",
    "#The prediction was not that good. Then I modified the number of epochs to 200 and tested the model again. \n",
    "#Accuracy had improved slightly, but figured I’d give it one more try. Finally, at 500 epochs \n",
    "#I found acceptable prediction accuracy.\n",
    "\n",
    "#The fit method takes three parameters; namely, x, y, and number of epochs. \n",
    "#During model training, if all the batches of data are seen by the model once, \n",
    "#we say that one epoch has been completed.\n",
    "\n",
    "# Add an early stopping callback\n",
    "es = keras.callbacks.EarlyStopping(\n",
    "    monitor='loss', \n",
    "    mode='min', \n",
    "    patience = 120, \n",
    "    restore_best_weights = True, \n",
    "    verbose=1)\n",
    "# Add a checkpoint where loss is minimum, and save that model\n",
    "mc = keras.callbacks.ModelCheckpoint('best_model.SB', monitor='loss', \n",
    "                     mode='min',  verbose=1, save_best_only=True)\n",
    "\n",
    "historyData = model.fit(xarray,yarray,epochs=800,callbacks=[es])\n",
    "\n",
    "loss_hist = historyData.history['loss']\n",
    "#The above line will return a dictionary, access it's info like this:\n",
    "best_epoch = np.argmin(historyData.history['loss']) +1\n",
    "print ('best epoch = ', best_epoch)\n",
    "print('smallest loss =', np.min(loss_hist))\n",
    "\n",
    "model.save('./best_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "52f04627-453d-4ade-a65a-d632db1212e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# This line of code can be used to reconstruct the saved model.\n",
    "\n",
    "recon_model = keras.models.load_model(\"best_model\")\n",
    "\n",
    "# the name of the model is now \"recon_model\". You can then use this model to do predictions for comparisons.\n",
    "# See the previous project for code to do the comparisons.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "791dffa1-53a4-4800-9fc1-321081c014a6",
   "metadata": {},
   "source": [
    "### Task 1.1 e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "a19ec5aa-9d74-4c88-9ffd-ecf994895f62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicted powet output [0.9399989, 0.9524178, 1.0000718, 0.90263855, 0.8617271, 0.8904225, 0.8910619, 0.97994804, 0.8913358, 0.96924335, 0.9194617, 1.0650342, 1.0184743, 0.9447638, 1.0528867, 1.0237355, 0.87774104, 1.0692959, 0.9587865, 0.91352046, 1.0408981, 0.9735674, 1.0909477, 0.89605975, 0.92169315, 1.003394, 0.9088704, 1.0152527, 0.9147075, 0.93626267, 1.0037266, 0.95097446, 0.99937016, 0.9847058, 0.9502697, 0.9424225, 0.90041023, 1.0390159, 0.9459386]\n",
      "mean absolute error: 0.04100514367772902\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib notebook\n",
    "%matplotlib inline\n",
    "\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "# predict output using the trained model for the training data\n",
    "YP=[]\n",
    "YD=[]\n",
    "for i in range(len(train_xarray)): \n",
    "    test = [[train_xarray[i][0], train_xarray[i][1], train_xarray[i][2]]]\n",
    "    testarray = np.array(test)\n",
    "    a3 = model.predict(testarray)\n",
    "    YP.append(a3[0][1])\n",
    "    YD.append(train_yarray[i][1])\n",
    "print('predicted powet output', YP)\n",
    "#mean absolute error calculation\n",
    "from sklearn.metrics import mean_absolute_error as mae\n",
    "error=mae(YD,YP)\n",
    "print('mean absolute error:', error)\n",
    "#comparision of predicted vs training data set\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "#plt.plot(x,x, color='green', linestyle='--')\n",
    "plt.scatter(YP,YD)\n",
    "plt.xlabel('Predicted power output', fontsize='16')\n",
    "plt.ylabel('training power output ', fontsize='16')\n",
    "plt.loglog()\n",
    "plt.grid()\n",
    "#plt.plot(x,x, color='green', linestyle='-')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ad2997b-8edf-43e5-87dd-13195367a1ff",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
